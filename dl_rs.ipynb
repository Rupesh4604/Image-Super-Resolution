{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":96985,"databundleVersionId":11709207,"sourceType":"competition"}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T13:24:33.189217Z","iopub.execute_input":"2025-04-19T13:24:33.189470Z","iopub.status.idle":"2025-04-19T13:24:33.194655Z","shell.execute_reply.started":"2025-04-19T13:24:33.189445Z","shell.execute_reply":"2025-04-19T13:24:33.193785Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, channels):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n        \n    def forward(self, x):\n        residual = x\n        out = self.relu(self.conv1(x))\n        out = self.conv2(out)\n        out += residual  # Skip connection\n        return out\n\nclass LightweightSR(nn.Module):\n    def __init__(self, num_blocks=8, feature_channels=64, upscale_factor=4):\n        super(LightweightSR, self).__init__()\n        \n        # Initial feature extraction\n        self.initial = nn.Sequential(\n            nn.Conv2d(3, feature_channels, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True)\n        )\n        \n        # Residual blocks\n        self.residual_blocks = nn.Sequential(\n            *[ResidualBlock(feature_channels) for _ in range(num_blocks)]\n        )\n        \n        # Global skip connection conv\n        self.conv_mid = nn.Conv2d(feature_channels, feature_channels, kernel_size=3, padding=1)\n        \n        # Upsampling - use pixel shuffle for efficiency\n        if upscale_factor == 4:\n            self.upscale = nn.Sequential(\n                nn.Conv2d(feature_channels, feature_channels*4, kernel_size=3, padding=1),\n                nn.PixelShuffle(2),\n                nn.Conv2d(feature_channels, feature_channels*4, kernel_size=3, padding=1),\n                nn.PixelShuffle(2)\n            )\n        else:  # Assuming upscale_factor == 2\n            self.upscale = nn.Sequential(\n                nn.Conv2d(feature_channels, feature_channels*4, kernel_size=3, padding=1),\n                nn.PixelShuffle(2)\n            )\n        \n        # Final reconstruction\n        self.final = nn.Conv2d(feature_channels, 3, kernel_size=3, padding=1)\n        \n    def forward(self, x):\n        initial = self.initial(x)\n        \n        out = self.residual_blocks(initial)\n        out = self.conv_mid(out)\n        out = out + initial  # Global residual learning\n        \n        out = self.upscale(out)\n        out = self.final(out)\n        \n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T13:24:33.196381Z","iopub.execute_input":"2025-04-19T13:24:33.197504Z","iopub.status.idle":"2025-04-19T13:24:37.047982Z","shell.execute_reply.started":"2025-04-19T13:24:33.197479Z","shell.execute_reply":"2025-04-19T13:24:37.047269Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\nclass SRDataset(Dataset):\n    def __init__(self, lr_paths, hr_paths=None, patch_size=96, is_train=True):  # Increased from 64 to 96\n        self.lr_paths = lr_paths\n        self.hr_paths = hr_paths\n        self.patch_size = patch_size\n        self.is_train = is_train\n        \n    def __len__(self):\n        return len(self.lr_paths)\n    \n    def __getitem__(self, idx):\n        # Load LR image\n        lr_img = cv2.imread(self.lr_paths[idx])\n        lr_img = cv2.cvtColor(lr_img, cv2.COLOR_BGR2RGB)\n        \n        if self.hr_paths:\n            # Load HR image\n            hr_img = cv2.imread(self.hr_paths[idx])\n            hr_img = cv2.cvtColor(hr_img, cv2.COLOR_BGR2RGB)\n            \n            # Random crop for training\n            if self.is_train:\n                h, w = lr_img.shape[:2]\n                \n                # Ensure we have enough space for a crop\n                if h < self.patch_size or w < self.patch_size:\n                    # If image is too small, resize it to be at least patch_size\n                    scale_factor = max(self.patch_size / h, self.patch_size / w) * 1.1\n                    new_h, new_w = int(h * scale_factor), int(w * scale_factor)\n                    lr_img = cv2.resize(lr_img, (new_w, new_h))\n                    \n                    # Resize HR accordingly\n                    scale = hr_img.shape[0] // h  # Scale between HR and LR\n                    hr_img = cv2.resize(hr_img, (new_w * scale, new_h * scale))\n                    \n                    h, w = lr_img.shape[:2]  # Update dimensions\n                \n                # Random crop\n                x = np.random.randint(0, w - self.patch_size)\n                y = np.random.randint(0, h - self.patch_size)\n                \n                # Adjust for HR image\n                scale = hr_img.shape[0] // lr_img.shape[0]\n                lr_img = lr_img[y:y+self.patch_size, x:x+self.patch_size]\n                hr_img = hr_img[y*scale:(y+self.patch_size)*scale, x*scale:(x+self.patch_size)*scale]\n                \n                # Random flip\n                if np.random.random() > 0.5:\n                    lr_img = np.flip(lr_img, axis=1)\n                    hr_img = np.flip(hr_img, axis=1)\n        \n        # Convert to tensor and normalize to [-1, 1]\n        lr_tensor = torch.from_numpy(lr_img.transpose(2, 0, 1).astype(np.float32)) / 127.5 - 1.0\n        \n        if self.hr_paths:\n            hr_tensor = torch.from_numpy(hr_img.transpose(2, 0, 1).astype(np.float32)) / 127.5 - 1.0\n            return lr_tensor, hr_tensor\n        else:\n            return lr_tensor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T13:24:37.048757Z","iopub.execute_input":"2025-04-19T13:24:37.049087Z","iopub.status.idle":"2025-04-19T13:24:40.650651Z","shell.execute_reply.started":"2025-04-19T13:24:37.049062Z","shell.execute_reply":"2025-04-19T13:24:40.650115Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch.cuda.amp as amp\nfrom tqdm.notebook import tqdm\n\ndef train_model(model, train_loader, val_loader, epochs=50, lr=1e-4):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n    \n    criterion = nn.L1Loss()  # L1 is better for image SR\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    \n    # Cosine annealing scheduler for faster convergence\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n    \n    # Mixed precision for efficiency\n    scaler = amp.GradScaler()\n    \n    best_psnr = 0\n    \n    for epoch in range(epochs):\n        model.train()\n        epoch_loss = 0\n        \n        with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\") as pbar:\n            for lr_imgs, hr_imgs in pbar:\n                lr_imgs = lr_imgs.to(device)\n                hr_imgs = hr_imgs.to(device)\n                \n                optimizer.zero_grad()\n                \n                # Mixed precision training - fixed syntax\n                with torch.amp.autocast('cuda'):\n                    sr_imgs = model(lr_imgs)\n                    loss = criterion(sr_imgs, hr_imgs)\n                \n                # Scale gradients and optimize\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n                \n                epoch_loss += loss.item()\n                pbar.set_postfix(loss=f\"{epoch_loss/len(pbar):.4f}\")\n        \n        scheduler.step()\n        \n        # Validate\n        model.eval()\n        val_psnr = evaluate_psnr(model, val_loader, device)\n        print(f\"Validation PSNR: {val_psnr:.2f} dB\")\n        \n        # Save if best\n        if val_psnr > best_psnr:\n            best_psnr = val_psnr\n            torch.save(model.state_dict(), \"best_sr_model_1.pth\")\n            \n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T13:24:40.651484Z","iopub.execute_input":"2025-04-19T13:24:40.651897Z","iopub.status.idle":"2025-04-19T13:24:40.776295Z","shell.execute_reply.started":"2025-04-19T13:24:40.651864Z","shell.execute_reply":"2025-04-19T13:24:40.775765Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n\ndef evaluate_psnr(model, val_loader, device):\n    model.eval()\n    psnr_values = []\n    ssim_values = []\n    \n    with torch.no_grad():\n        for lr_imgs, hr_imgs in val_loader:\n            lr_imgs = lr_imgs.to(device)\n            hr_imgs = hr_imgs.cpu().numpy().transpose(0, 2, 3, 1)\n            \n            # Denormalize images from [-1, 1] to [0, 255]\n            hr_imgs = (hr_imgs + 1) * 127.5\n            \n            sr_imgs = model(lr_imgs).cpu().numpy().transpose(0, 2, 3, 1)\n            sr_imgs = np.clip((sr_imgs + 1) * 127.5, 0, 255)\n            \n            # Calculate metrics\n            for i in range(lr_imgs.size(0)):\n                psnr_values.append(peak_signal_noise_ratio(hr_imgs[i], sr_imgs[i], data_range=255))\n                # Fix the SSIM call - specify channel_axis instead of multichannel\n                ssim_values.append(structural_similarity(\n                    hr_imgs[i], \n                    sr_imgs[i], \n                    channel_axis=2,  # Specify that channels are on axis 2\n                    data_range=255,\n                    win_size=7  # Explicitly set window size to be smaller\n                ))\n                \n    avg_psnr = np.mean(psnr_values)\n    avg_ssim = np.mean(ssim_values)\n    \n    # Calculate joint metric (as per competition rules)\n    joint_metric = 40 * avg_ssim + avg_psnr\n    \n    return joint_metric","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T13:24:40.776888Z","iopub.execute_input":"2025-04-19T13:24:40.777075Z","iopub.status.idle":"2025-04-19T13:24:41.354422Z","shell.execute_reply.started":"2025-04-19T13:24:40.777059Z","shell.execute_reply":"2025-04-19T13:24:41.353660Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import os\nimport glob\nfrom sklearn.model_selection import train_test_split\n\n# Path setup\ntrain_lr_dir = \"/kaggle/input/gnr638/train-kaggle/train-kaggle/lr\"\ntrain_hr_dir = \"/kaggle/input/gnr638/train-kaggle/train-kaggle/hr\"  # If available\n\n# Get file lists\nlr_files = sorted(glob.glob(os.path.join(train_lr_dir, \"*.png\")))\nhr_files = sorted(glob.glob(os.path.join(train_hr_dir, \"*.png\"))) if os.path.exists(train_hr_dir) else None\n\n# Create train/val split\ntrain_lr, val_lr = train_test_split(lr_files, test_size=0.1, random_state=42)\ntrain_hr, val_hr = train_test_split(hr_files, test_size=0.1, random_state=42) if hr_files else (None, None)\n\n# Create datasets with larger patch size\ntrain_dataset = SRDataset(train_lr, train_hr, patch_size=96, is_train=True)\nval_dataset = SRDataset(val_lr, val_hr, is_train=False)\n\n# Create data loaders - small batch size to save memory\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=4)\n\n# Create model\nmodel = LightweightSR(num_blocks=8, feature_channels=64)\n\n# Train model\ntrain_model(model, train_loader, val_loader, epochs=10, lr=1e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T13:24:41.355214Z","iopub.execute_input":"2025-04-19T13:24:41.355658Z","iopub.status.idle":"2025-04-19T14:39:52.398012Z","shell.execute_reply.started":"2025-04-19T13:24:41.355630Z","shell.execute_reply":"2025-04-19T14:39:52.397223Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/4201916501.py:15: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = amp.GradScaler()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/10:   0%|          | 0/254 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"769ce7968938457fa1c82658de3343e6"}},"metadata":{}},{"name":"stdout","text":"Validation PSNR: 49.59 dB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/10:   0%|          | 0/254 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc57e003a65249f09d88d0510f240256"}},"metadata":{}},{"name":"stdout","text":"Validation PSNR: 52.72 dB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3/10:   0%|          | 0/254 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b062c9ce84ce49d6920c2c58619bfec6"}},"metadata":{}},{"name":"stdout","text":"Validation PSNR: 52.80 dB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4/10:   0%|          | 0/254 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"892b8e0f0b6a492c80233c39ed031e10"}},"metadata":{}},{"name":"stdout","text":"Validation PSNR: 52.48 dB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5/10:   0%|          | 0/254 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fa34134d0fe44bd8daf68ee350a3f6c"}},"metadata":{}},{"name":"stdout","text":"Validation PSNR: 53.51 dB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 6/10:   0%|          | 0/254 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"666d523d40ba44e9a7114c8a2cff61d0"}},"metadata":{}},{"name":"stdout","text":"Validation PSNR: 53.76 dB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 7/10:   0%|          | 0/254 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c4892c01f41494abe3cefe403f67e05"}},"metadata":{}},{"name":"stdout","text":"Validation PSNR: 53.83 dB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 8/10:   0%|          | 0/254 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f8dabc7cf2e48e19da1fabb3cc36716"}},"metadata":{}},{"name":"stdout","text":"Validation PSNR: 54.15 dB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 9/10:   0%|          | 0/254 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b39e770cfd440f58d469b3c66068801"}},"metadata":{}},{"name":"stdout","text":"Validation PSNR: 53.99 dB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 10/10:   0%|          | 0/254 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6156ac561b9241e5ada2bd5b45b0c667"}},"metadata":{}},{"name":"stdout","text":"Validation PSNR: 54.17 dB\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"LightweightSR(\n  (initial): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n  )\n  (residual_blocks): Sequential(\n    (0): ResidualBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    )\n    (1): ResidualBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    )\n    (2): ResidualBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    )\n    (3): ResidualBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    )\n    (4): ResidualBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    )\n    (5): ResidualBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    )\n    (6): ResidualBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    )\n    (7): ResidualBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    )\n  )\n  (conv_mid): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (upscale): Sequential(\n    (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): PixelShuffle(upscale_factor=2)\n    (2): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): PixelShuffle(upscale_factor=2)\n  )\n  (final): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"import base64\nimport pandas as pd\nfrom tqdm import tqdm\n\ndef generate_submission(model, test_dir, output_csv_file):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    model.eval()\n    \n    # Get test files\n    test_files = sorted(glob.glob(os.path.join(test_dir, \"*.png\")))\n    encoded_images = []\n    \n    for file_path in tqdm(test_files):\n        # Load and preprocess image\n        img = cv2.imread(file_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        # Convert to tensor\n        lr_tensor = torch.from_numpy(img.transpose(2, 0, 1).astype(np.float32)) / 127.5 - 1.0\n        lr_tensor = lr_tensor.unsqueeze(0).to(device)\n        \n        # Inference\n        with torch.no_grad():\n            sr_tensor = model(lr_tensor)\n        \n        # Convert back to image\n        sr_img = sr_tensor.squeeze(0).cpu().numpy().transpose(1, 2, 0)\n        sr_img = np.clip((sr_img + 1) * 127.5, 0, 255).astype(np.uint8)\n        sr_img = cv2.cvtColor(sr_img, cv2.COLOR_RGB2BGR)\n        \n        # Save temporarily and encode\n        temp_path = f\"temp_{os.path.basename(file_path)}\"\n        cv2.imwrite(temp_path, sr_img)\n        \n        # Encode to base64\n        with open(temp_path, \"rb\") as img_file:\n            encoded_img = base64.b64encode(img_file.read()).decode('utf-8')\n        \n        os.remove(temp_path)  # Clean up\n        \n        encoded_images.append({\n            'id': os.path.basename(file_path),\n            'Encoded_Image': encoded_img\n        })\n    \n    # Create submission DataFrame\n    df = pd.DataFrame(encoded_images)\n    df.to_csv(output_csv_file, index=False)\n    print(f\"Submission saved to {output_csv_file}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T14:39:52.400191Z","iopub.execute_input":"2025-04-19T14:39:52.400464Z","iopub.status.idle":"2025-04-19T14:39:52.673027Z","shell.execute_reply.started":"2025-04-19T14:39:52.400442Z","shell.execute_reply":"2025-04-19T14:39:52.672488Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# First make sure your model is loaded with the best weights\nmodel.load_state_dict(torch.load(\"best_sr_model_1.pth\"))\n\n# Define your test directory and output path\ntest_dir = \"/kaggle/input/gnr638/lr/lr\"  # Directory containing test LR images\noutput_csv_file = \"/kaggle/working/submission_latest.csv\"  # Where to save the submission file\n\n# Call the submission function\ngenerate_submission(model, test_dir, output_csv_file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T14:39:52.673625Z","iopub.execute_input":"2025-04-19T14:39:52.674065Z","iopub.status.idle":"2025-04-19T14:42:22.443273Z","shell.execute_reply.started":"2025-04-19T14:39:52.674033Z","shell.execute_reply":"2025-04-19T14:42:22.442595Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/1802956372.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"best_sr_model_1.pth\"))\n100%|██████████| 500/500 [01:50<00:00,  4.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Submission saved to /kaggle/working/submission_latest.csv\n","output_type":"stream"}],"execution_count":8}]}